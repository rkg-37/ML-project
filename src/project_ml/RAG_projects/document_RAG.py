# You can use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
from langchain_core.documents import Document
from typing import Iterator
warnings.warn = warn
warnings.filterwarnings('ignore')
from uuid import uuid4
import re
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter , CharacterTextSplitter
from langchain_core.vectorstores import VectorStore
from langchain_community.vectorstores import FAISS , chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_classic.memory import ConversationBufferMemory
import wget
from langchain_ollama import OllamaEmbeddings , ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import SystemMessagePromptTemplate , ChatMessagePromptTemplate
from langchain_core.runnables import RunnableLambda , RunnableSequence , RunnablePassthrough
from langchain_ollama  import ChatOllama


def download_file(url, filename):
    if url is None:
        url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/6JDbUb_L3egv_eOkouY71A.txt'
    try:
        wget.download(url, out = filename)
        print(f"File downloaded successfully to {filename}")
    except Exception as e:
        print(f"An error occurred while downloading the file: {e}")
        
def read_txt_file(filename):
    try:
        with open(filename, 'r') as file:
            content = file.read()
        return content
    except Exception as e:
        print(f"An error occurred while reading the file: {e}")
        return None
    
def load_txt_file(filename):
    try:
        return TextLoader(filename).lazy_load()
    except Exception as e:
        print(f"An error occurred while loading the file: {e}")
        return None
    
def split_txt_text(text : list[Document], chunk_size=1000, chunk_overlap=200):
    try:
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        return text_splitter.split_documents(text)
    except Exception as e:
        print(f"An error occurred while splitting the text: {e}")
        return None
    
def create_embeddings(model_name='granite-embedding:278m'):
    try:
        return OllamaEmbeddings(model=model_name)
    except Exception as e:
        print(f"An error occurred while creating embeddings: {e}")
        return None
    
def create_vector_store(texts : list[Document], embeddings):
    try:
        return chroma.Chroma.from_documents(texts, embeddings)
    except Exception as e:
        print(f"An error occurred while creating the vector store: {e}")
        return None

def create_retriever(vector_store):
    try:
        return vector_store.as_retriever()
    except Exception as e:
        print(f"An error occurred while creating the retriever: {e}")
        return None

def create_llm(model_name="llama3.2:latest" , verbose=True):
    try:
        return ChatOllama(model=model_name , verbose=verbose)
    except Exception as e:
        print(f"An error occurred while creating the LLM: {e}")
        return None
    
def format_docs(docs : list[Document]):
    try:
        formatted_docs = []
        for doc in docs:
            formatted_doc = f"Document ID: {doc.metadata.get('id', 'N/A')}\nContent: {doc.page_content}\n"
            formatted_docs.append(formatted_doc)
        return "\n\n".join(formatted_docs)
    except Exception as e:
        print(f"An error occurred while formatting the documents: {e}")
        return None

def create_rag_chain(llm, retriever):
    try:
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system", 
                    "You are a RAG application assistant. Use the following context to answer the user's question: \n\n{context}"
                ),
                (
                    "human", 
                    "{user_input}"
                ),
            ]
        )
        rag_chain = (
            {"context": retriever | format_docs, "user_input": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        return rag_chain
    except Exception as e:
        print(f"An error occurred while creating the RAG chain: {e}")
        return None
    
def main():
    url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/6JDbUb_L3egv_eOkouY71A.txt'
    filename = 'C:\\Users\\rajka\\Desktop\\project_ml\\src\\project_ml\\RAG\\code_of_conduct.txt'
    download_file(url, filename)
    text = load_txt_file(filename)
    if text is not None:
        texts = split_txt_text(text)
        if texts is not None:
            embeddings_ollama = create_embeddings()
            if embeddings_ollama is not None:
                docsearch_ollama = create_vector_store(texts, embeddings_ollama)
                if docsearch_ollama is not None:
                    db = create_retriever(docsearch_ollama)
                    if db is not None:
                        llm = create_llm()
                        if llm is not None:
                            # Create the RAG chain
                            rag_chain = create_rag_chain(llm, db)
                            query = "What is the code of conduct summary?"
                            response = rag_chain.invoke(query)
                            print(response)
                            
if __name__ == "__main__":
    main()