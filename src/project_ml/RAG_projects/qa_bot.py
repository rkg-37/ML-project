# You can use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
from langchain_core.documents import Document
from typing import Iterator
warnings.warn = warn
warnings.filterwarnings('ignore')
from uuid import uuid4
import re   
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter , CharacterTextSplitter
from langchain_core.vectorstores import VectorStore
from langchain_community.vectorstores import FAISS , chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_classic.memory import ConversationBufferMemory
import wget
from langchain_ollama import OllamaEmbeddings , ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import SystemMessagePromptTemplate , ChatMessagePromptTemplate
from langchain_core.runnables import RunnableLambda , RunnableSequence , RunnablePassthrough
from langchain_ollama  import ChatOllama
from langchain_community.document_loaders import PyPDFLoader
import gradio as gr

# By the end of this project, you will be able to:
# * Combine multiple components, such as document loaders, text splitters, 
# embedding models, and vector databases, to construct a fully functional QA bot
# * Leverage LangChain and LLMs to solve the problem of retrieving and answering 
# questions based on content from large PDF documents

def create_llm(model_name="llama3.2:latest" , verbose=True):
    try:
        print(f"Creating LLM with model: {model_name}...")
        return ChatOllama(model=model_name , verbose=verbose)
    except Exception as e:
        print(f"An error occurred while creating the LLM: {e}")
        return None

## Document loader
def document_loader(filename):
    try: 
        print("Loading document...")    
        return PyPDFLoader(filename).load()
    except Exception as e:  
        print(f"An error occurred while loading the document: {e}")
        return None
    
def text_splitter(chunk_size=1000, chunk_overlap=200):
    try:
        print("Splitting text into chunks...")  
        return RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap , length_function=len)              
    except Exception as e:
        print(f"An error occurred while creating the text splitter: {e}")
        return None
    
## Vector db
def vector_database(chunks):
    print("Creating vector database...")
    embedding_model = OllamaEmbeddings(model='granite-embedding:278m')
    vectordb = chroma.Chroma.from_documents(chunks, embedding_model)
    return vectordb

def format_docs(docs):
    formatted_docs = []
    for doc in docs:
        formatted_doc = f"Document ID: {doc.metadata.get('id', 'N/A')}\nContent: {doc.page_content}\n"
        formatted_docs.append(formatted_doc)
    return "\n\n".join(formatted_docs)

def prompt_template():
    prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system", 
                    "You are a RAG application assistant. Use the following context to answer the user's question: \n\n{context}"
                ),
                (
                    "human", 
                    "{user_input}"
                ),
            ]
        )
    return prompt


## Retriever
def retriever(filename):
    content = document_loader(filename)
    spliter = RunnableLambda(lambda x : text_splitter().create_documents([x]))
    chunks = spliter.invoke(format_docs(content))
    vectordb = vector_database(chunks)
    retriever = vectordb.as_retriever()
    return retriever


def retriver_qa_bot( filename , query, model_name="llama3.2:latest" , verbose=True):
    print(f"filename: {filename}\n"f"query: {query}\n")
    retriever_instance = retriever(filename)
    prompt = prompt_template()
    llm = create_llm(model_name=model_name , verbose=verbose)
    qa_chain = ({"context":retriever_instance | format_docs , "user_input":RunnablePassthrough()} | prompt | llm | StrOutputParser()) 
    response = qa_chain.invoke(query)
    return response




def main():

    # Create Gradio interface
    
    rag_application = gr.Interface(
        fn=retriver_qa_bot,
        flagging_mode="never",
        inputs=[
            gr.File(label="Upload PDF File", file_count="single", file_types=['.pdf'], type="filepath"),  # Drag and drop file upload
            gr.Textbox(label="Input Query", lines=2, placeholder="Type your question here...")
        ],
        outputs=gr.Textbox(label="Output"),
        title="RAG Chatbot",
        description="Upload a PDF document and ask any question. The chatbot will try to answer using the provided document."
    )
    
    rag_application.launch(server_name="127.0.0.22", server_port= 7860)
    
    
if __name__ == "__main__":
    main()    

    